{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will look at some posts on Stack Overflow during the year of 2015 and measure the similarity of users by looking at the types of questions they answer. Do not delete the output of your code cells. This assignment is to be completed **INDIVIDUALLY** and it is due on **September 30**.\n",
    "\n",
    "Please update the README with your BU username."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working on the notebook, let's make sure that everything is setup properly. You should have downloaded and installed\n",
    "* [Anaconda](https://store.continuum.io/cshop/anaconda/)\n",
    "* [Git](http://git-scm.com/downloads)\n",
    "\n",
    "If you are working from the undergraduate lab (on a linux machine) these are both installed, but you need to follow the instructions [from here](https://github.com/mcrovella/CS505-Computational-Tools-for-Data-Science/blob/master/2-Getting-Started.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a sample request to retrieve some questions posted on Stack Exchange on the first day of 2015. Documentation of the Stack Exchange API can be found [here](https://api.stackexchange.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = 1420070400 # 01-01-2015 at 00:00:00\n",
    "end_time   = 1420156800 # 01-02-2015 at 00:00:00\n",
    "\n",
    "response = requests.get(\"https://api.stackexchange.com/2.2/questions?pagesize=100\" +\n",
    "                        \"&fromdate=\" + str(start_time) + \"&todate=\" + str(end_time) +\n",
    "                        \"&order=asc&sort=creation&site=stackoverflow\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates in the Stack Exchange API are in [unix epoch time](https://en.wikipedia.org/wiki/Unix_time). The format for the request string is specified [here](https://api.stackexchange.com/docs/questions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to print the response that Stack Exchange returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not possible to read the raw response. Instead, we need to decode the raw response as JSON and use the `json` library to print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_response = response.json()\n",
    "\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily see that the response consists of a list of question items. For each of these items, we get information about its attributes such as its `creation_date`, `answer_count`, `owner`, `title`, etc.\n",
    "\n",
    "Notice that has_more is true. To get more items, we can [request the next page](https://api.stackexchange.com/docs/paging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Parsing the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we practice some of the basic Python tools that we learned in class and the powerful string handling methods that Python offers. Our goal is to be able to pick the interesting parts of the response and transform them in a format that will be useful to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's isolate the creation_date in the response. Fill in the rest of the ```print_creation_dates_json()``` function that reads the response and prints the creation dates. Notice that a JSON object is basically a dictionary. **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_creation_dates_json(response):\n",
    "    \"\"\"\n",
    "    Prints the creation_date of all the questions in the response.\n",
    "    \n",
    "    Parameters:\n",
    "        response: Response object\n",
    "    \"\"\"\n",
    "    json_response=response.json()\n",
    "    for i in range(0, len(json_response[\"items\"])):\n",
    "\t\tprint(json_response[\"items\"][i]['creation_date'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that calls the ```print_creation_dates_json()``` function to print out all the creation dates of questions posted on the first day in 2015. Please be aware of Stack Exchange's [rate limit](https://api.stackexchange.com/docs/throttle). **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = 1420070400 # 01-01-2015 at 00:00:00\n",
    "end_time   = 1420156800 # 01-02-2015 at 00:00:00\n",
    "\n",
    "\n",
    "i=1\n",
    "response = requests.get(\"https://api.stackexchange.com/2.2/questions?page=\"+str(i)+\"&pagesize=100\" + \"&fromdate=\" + str(start_time) + \"&todate=\" + str(end_time) +\"&order=asc&sort=creation&site=stackoverflow\")\n",
    "json_response = response.json()\n",
    "while(json_response[\"has_more\"]):\n",
    "\t\n",
    "\tresponse = requests.get(\"https://api.stackexchange.com/2.2/questions?page=\"+str(i)+\"&pagesize=100\" + \"&fromdate=\" + str(start_time) + \"&todate=\" + str(end_time) +\"&order=asc&sort=creation&site=stackoverflow\")\n",
    "\tprint_creation_dates_json(response)\n",
    "\ttime.sleep(3)\n",
    "\ti+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, we have downloaded the [data dump](https://drive.google.com/open?id=0B4bdB5WPHGsqTm9PZlBhRVQzRmc) for Stack Overflow's posts in 2015. The link is only visible to BU students, so you must be logged in to your BU email. Note that the XML file is 10GB. If you don't have space on your computer, you can download it into `/scratch` on one of the machines in the undergrad lab or you can download it onto a USB. You may want to work with a subset of this data at first, but your solution should be efficient enough to work with the whole dataset. For example, if you call `read()` on the whole dataset, you will get a `MemoryError`.\n",
    "\n",
    "Do not commit the data file. You may assume that we will place the data file in the same directory as your IPython Notebook, so provide a relative path when loading the data file.\n",
    "\n",
    "Write a function to parse out the questions posted in 2015. These are posts with `PostTypeId=1`. Make a `pandas DataFrame` with 4 columns: `Id`, `CreationDate`, `OwnerUserId`, and the first tag in `Tags`. Save the `DataFrame` to a file named `question_dataframe.csv` using `to_csv()`. You may also parse out the data to `question_dataframe.csv` first and then create the `pandas DataFrame` using `read_csv()`. **(10 pts)**\n",
    "\n",
    "Your code should run in a few minutes (not hours!). In a markdown cell, write down the approximate run time of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('stackoverflow-posts-2015.xml')\n",
    "root = tree.getroot()\n",
    "data =[]\n",
    "for child in root:\n",
    "\tif(child.attrib['PostTypeId']=='1'):\n",
    "\t\trow =[]\n",
    "\t\tif ('Id' in child.attrib and 'CreationDate' in child.attrib and 'OwnerUserId' in child.attrib and 'Tags' in child.attrib):\n",
    "\t\t\trow.extend([child.attrib['Id'],child.attrib['CreationDate'],child.attrib['OwnerUserId']])\n",
    "\t\t\trow.extend([child.attrib['Tags'][1:child.attrib['Tags'].find('>')]])\n",
    "\t\t\tdata.append(row)\n",
    "\n",
    "\n",
    "df=pandas.DataFrame({'Id':[row[0] for row in data],'CreationDate':[row[1] for row in data],'OwnerUserId':[row[2] for row in data],'Tags':[row[3] for row in data]})\n",
    "\n",
    "df.to_csv('question_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tackle our original problem. Write a function to measure the similarity of the top 100 users with the most answer posts. Compare the users based on the types of questions they answer. We will categorize a question by its first tag. You may choose to implement any one of the similarity/distance measures we discussed in class. **(30pts)**\n",
    "\n",
    "Note that answers are posts with `PostTypeId=2`. The ID of the question in answer posts is the `ParentId`.\n",
    "\n",
    "You may find the [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/feature_extraction.html) module helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('stackoverflow-posts-2015.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "date=[]\n",
    "for child in root:\n",
    "\tif(child.attrib['PostTypeId']=='2'):\n",
    "\t\trow=[]\n",
    "\t\tif('ParentId' in child.attrib):\n",
    "\t\t\tif('Id' in child.attrib):\n",
    "\t\t\t\tif('CreationDate' in child.attrib):\n",
    "\t\t\t\t\tif('OwnerUserId' in child.attrib):\n",
    "\t\t\t\t\t\trow.extend([child.attrib['Id'],child.attrib['CreationDate'],child.attrib['OwnerUserId'],child.attrib['ParentId']])\n",
    "\t\t\t\t\t\tdata.append(row)\n",
    "dfq=pd.DataFrame({'Id':[row[0] for row in data],'CreationDate':[row[1] for row in data],'OwnerUserId':[row[2] for row in data],'ParentId':[row[3] for row in data]})\n",
    "dfq.to_csv('answer_dataframe.csv')\n",
    "df['OwnerUserId']=df['OwnerUserId'].astype('int')\n",
    "df.merge(dfq, left_on='OwnerUserId', right_on='OwnerUserId',how='inner')\n",
    "df.groupby('OwnerUserId').count()['Id'].nlargest(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distance of the top 100 users using a [heatmap](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.heatmap.html). **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
